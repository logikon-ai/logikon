{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliberative Agent Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    #filename='logikon_chatbot.log',\n",
    "    #filemode='w',\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.DEBUG,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logikon\n",
    "\n",
    "logging.info(f\"Installed `logikon` module version: {logikon.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_openai import ChatOpenAI\n",
    "from logikon.backends.chat_models_with_grammar import create_logits_model\n",
    "\n",
    "inference_server_url = \"http://localhost:8000/v1\"\n",
    "model_id = \"openchat/openchat_3.5\"\n",
    "\n",
    "llm = create_logits_model(\n",
    "    model_id=model_id,\n",
    "    inference_server_url=inference_server_url,\n",
    "    api_key=\"EMPTY\",\n",
    "    llm_backend=\"VLLM\",\n",
    "    max_tokens=780,\n",
    "    temperature=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SERVER\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke(\"farmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "import gradio as gr\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "from logikon.guides.proscons.recursive_balancing_guide import RecursiveBalancingGuide, RecursiveBalancingGuideConfig\n",
    "\n",
    "guide_config = RecursiveBalancingGuideConfig(\n",
    "    expert_model = \"openchat/openchat_3.5\",\n",
    "    inference_server_url = \"http://localhost:8000/v1\",\n",
    "    api_key = \"EMPTY\"\n",
    ")\n",
    "guide = RecursiveBalancingGuide(tourist_llm=llm, config=guide_config)\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\"We're a nature-loving family with three kids, have some money left, and no plans \"\n",
    "     \"for next week-end. Should we visit Disneyland?\"),\n",
    "    \"Should I stop eating animals?\",\n",
    "    \"Bob needs a reliable and cheap car. Should he buy a Mercedes?\",\n",
    "    ('Gavin has an insurance policy that includes coverage for \"General Damages,\" '\n",
    "     'which includes losses from \"missed employment due to injuries that occur '\n",
    "     'under regular working conditions.\"\\n\\n'\n",
    "     'Gavin works as an A/C repair technician in a small town. One day, Gavin is '\n",
    "     'hired to repair an air conditioner located on the second story of a building. '\n",
    "     'Because Gavin is an experienced repairman, he knows that the safest way to '\n",
    "     'access the unit is with a sturdy ladder. While climbing the ladder, Gavin '\n",
    "     'loses his balance and falls, causing significant injury. Because of this, he '\n",
    "     'subsequently has to stop working for weeks. Gavin files a claim with his '\n",
    "     'insurance company for lost income.\\n\\n'\n",
    "     'Does Gavin\\'s insurance policy cover his claim for lost income?'),\n",
    "     \"How many arguments did you consider in your internal reasoning? (Brief answer, please.)\",\n",
    "     \"Did you consider any counterarguments in your internal reasoning?\",\n",
    "     \"From all the arguments you considered and assessed, which one is the most important?\",\n",
    "     \"Did you refute any arguments or reasons for lack of plausibility?\"\n",
    "]\n",
    "\n",
    "\n",
    "def add_details(response: str, reasoning: str, svg_argmap: str) -> str:\n",
    "    \"\"\"Add reasoning details to the response message shown in chat.\"\"\"\n",
    "    response_with_details = (\n",
    "        f\"<p>{response}</p>\"\n",
    "        '<details id=\"reasoning\">'\n",
    "        \"<summary><i>Internal reasoning trace</i></summary>\"\n",
    "        f\"<code>{reasoning}</code></details>\"\n",
    "        '<details id=\"svg_argmap\">'\n",
    "        \"<summary><i>Argument map</i></summary>\"\n",
    "        f\"\\n<div>\\n{svg_argmap}\\n</div>\\n</details>\"\n",
    "    )\n",
    "    return response_with_details\n",
    "\n",
    "\n",
    "def get_details(response_with_details: str) -> Tuple[str, dict[str, str]]:\n",
    "    \"\"\"Extract response and details from response_with_details shown in chat.\"\"\"\n",
    "    if \"<details id=\" not in response_with_details:\n",
    "        return response_with_details, {}\n",
    "    details_dict = {}\n",
    "    response, *details_raw = response_with_details.split('<details id=\"')\n",
    "    for details in details_raw:\n",
    "        details_id, details_content = details.split('\"', maxsplit=1)\n",
    "        details_content = details_content.strip()\n",
    "        if details_content.endswith(\"</code></details>\"):\n",
    "            details_content = details_content.split(\"<code>\")[1].strip()\n",
    "            details_content = details_content[:-len(\"</code></details>\")].strip()\n",
    "        elif details_content.endswith(\"</div></details>\"):\n",
    "            details_content = details_content.split(\"<div>\")[1].strip()\n",
    "            details_content = details_content[:-len(\"</div></details>\")].strip()\n",
    "        else:\n",
    "            logging.warning(f\"Unrecognized details content: {details_content}\")\n",
    "            details_content = \"UNRECOGNIZED DETAILS CONTENT\"\n",
    "        details_dict[details_id] = details_content\n",
    "    return response, details_dict\n",
    "\n",
    "\n",
    "def remove_links_svg(svg):\n",
    "    svg = svg.replace(\"</a>\",\"\")\n",
    "    svg = svg.replace(\"\\n\\n\",\"\\n\")\n",
    "    regex = r\"<a xlink[^>]*>\"\n",
    "    svg = re.sub(regex, \"\", svg, count=0, flags=re.MULTILINE)\n",
    "    return svg\n",
    "\n",
    "\n",
    "def resize_svg(svg, max_width=800):\n",
    "    regex = r\"<svg width=\\\"(?P<width>[\\d]+)pt\\\" height=\\\"(?P<height>[\\d]+)pt\\\"\"\n",
    "    match = next(re.finditer(regex, svg, re.MULTILINE))\n",
    "    width = int(match.group(\"width\"))\n",
    "    height = int(match.group(\"height\"))\n",
    "    if width <= max_width:\n",
    "        return svg\n",
    "\n",
    "    scale = max_width / width\n",
    "    s_width = round(scale * width)\n",
    "    s_height = round(scale * height)\n",
    "    s_svg = svg.replace(match.group(), f'<svg width=\"{s_width}pt\" height=\"{s_height}pt\"')\n",
    "    return s_svg\n",
    "\n",
    "def postprocess_svg(svg):\n",
    "    svg = \"<svg\" + svg.split(\"<svg\", maxsplit=1)[1]\n",
    "    svg = remove_links_svg(svg)\n",
    "    svg = resize_svg(svg, max_width=800)\n",
    "    return svg\n",
    "\n",
    "async def predict(message, history):\n",
    "    \"\"\"Predict the response for the given message.\n",
    "    Args:\n",
    "    message: dict, the message to predict the response for.\n",
    "    history: list, the session-state history as shown to user in the chat interface.\n",
    "\n",
    "    Returns:\n",
    "    str, the predicted response for the given message\n",
    "    \"\"\"\n",
    "    history_langchain_format = []  # History in LangChain format, as shown to the LLM\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        response, details = get_details(ai)\n",
    "        logging.debug(f\"Details: {details}\")\n",
    "        history_langchain_format.append(AIMessage(content=response))\n",
    "        if \"reasoning\" in details:\n",
    "            content = f\"Internal reasoning trace (hidden from user): {details['reasoning']}\"\n",
    "            history_langchain_format.append(AIMessage(content=content))\n",
    "    logging.debug(f\"Message: {message}\")\n",
    "    logging.debug(f\"History: {history}\")\n",
    "    history_langchain_format.append(HumanMessage(content=message[\"text\"]))\n",
    "\n",
    "    if len(history_langchain_format) <= 1:\n",
    "        # use guide always and exclusively at first turn\n",
    "        response, artifacts = await guide(message[\"text\"])\n",
    "        svg = postprocess_svg(artifacts[\"svg\"])\n",
    "        response = add_details(response, artifacts[\"protocol\"], svg)\n",
    "\n",
    "    else:\n",
    "        response = llm.invoke(history_langchain_format).content\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "bot = gr.ChatInterface(\n",
    "    predict,\n",
    "    title=\"Logikon's Deliberative Agent Bot (Guided Reasoning Demo)\",\n",
    "    multimodal=True,\n",
    "    examples=[{\"text\": e, \"files\":[]} for e in EXAMPLES]\n",
    ")\n",
    "bot.launch(auth=(\"x123\",\"123\"), share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
